# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tw0UgtyLnYr3Rm47JjponjLGwnNGvzD2

Using BART
"""

import pandas as pd
from transformers import pipeline

df=pd.read_csv("/content/temp_out_1.csv")

#df

df1=df.drop(['id','Unnamed: 0'],axis=1)

#df1

pipe = pipeline(model="facebook/bart-large-mnli")

lst=[]
for i in df1['name']:
  if i not in lst:
    lst.append(i)
len(lst)

output=pipe("Tel. No.: 011-23253488 Website: www.mobius.com; Email Id: corpcomm@mobius.com; Email Id for investors: investors@dabur.com'",
     candidate_labels=lst,
 )

print("Top 3 probable labels:",'\n',output['labels'][:3])

"""Using SVM"""

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove*.zip

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np
import nltk
from nltk import word_tokenize
from gensim.scripts.glove2word2vec import glove2word2vec
from gensim.models import KeyedVectors

tokenizer = Tokenizer()
tokenizer.fit_on_texts(df['Explanation'])

word_index = tokenizer.word_index
vocab_size = len(word_index)
sequences = tokenizer.texts_to_sequences(df['explanation'])
padded_seq = pad_sequences(sequences, maxlen=131, padding='post', truncating='post')

embedding_index = {}
with open('glove.6B.100d.txt', encoding='utf-8') as f:
  for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embedding_index[word] = coefs

embedding_matrix = np.zeros((vocab_size+1, 100))
for word, i in word_index.items():
  embedding_vector = embedding_index.get(word)
  if embedding_vector is not None:
      embedding_matrix[i] = embedding_vector

glove2word2vec("/content/glove.6B.100d.txt", "glove.6B.100d.word2vec")
model = KeyedVectors.load_word2vec_format("glove.6B.100d.word2vec", binary=False)

def get_embeddings(model,sent):
    vec = np.array([model[word] if word in model else np.zeros((100)) for word in sent])
    vec = vec.sum(axis=0)
    return vec

def embedding(model,strng):
  res=np.array([get_embeddings(model,sent) for sent in [word_tokenize(strng)]])
  return res



vec = np.array([get_embeddings(model,sent) for sent in [word_tokenize(i) for i in df['explanation']]])

from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC

labels=LabelEncoder().fit_transform(df['name'])
svc=SVC()
svc.fit(vec,labels)

ypred=svc.predict(vec)

ypred

list(labels).index(16)

df['name'][124]

